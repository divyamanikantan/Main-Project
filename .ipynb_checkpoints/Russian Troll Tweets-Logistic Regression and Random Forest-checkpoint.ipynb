{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import unicodedata\n",
    "import json\n",
    "import random\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('tweets-2016-10000-textonly.txt', 'r',encoding=\"utf8\")\n",
    "lines = f.readlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "labels = []\n",
    "\n",
    "len_train = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@GStephanopoulos  Bernie Sanders on your show ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's 1:00am PST/4:00am EST. Do you know where ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#DumpTrump\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Look at this bullshit!��� SO topical! everybod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was a #BasementDwellers. Lived out of my car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8968</th>\n",
       "      <td>#Debates2016\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8969</th>\n",
       "      <td>Live from New York, it's a Trump-Clinton remat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8970</th>\n",
       "      <td>Same shit But now in color today #BlackLivesMa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8971</th>\n",
       "      <td>#TrumpTaxes\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8972</th>\n",
       "      <td>Trump Clinton devrait être en prison https://t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8973 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  labels\n",
       "0     @GStephanopoulos  Bernie Sanders on your show ...       0\n",
       "1     It's 1:00am PST/4:00am EST. Do you know where ...       0\n",
       "2                                          #DumpTrump\\n       0\n",
       "3     Look at this bullshit!��� SO topical! everybod...       1\n",
       "4     I was a #BasementDwellers. Lived out of my car...       0\n",
       "...                                                 ...     ...\n",
       "8968                                     #Debates2016\\n       0\n",
       "8969  Live from New York, it's a Trump-Clinton remat...       0\n",
       "8970  Same shit But now in color today #BlackLivesMa...       1\n",
       "8971                                      #TrumpTaxes\\n       0\n",
       "8972  Trump Clinton devrait être en prison https://t...       0\n",
       "\n",
       "[8973 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('IRAhandle_tweets_1.csv', newline='', encoding=\"utf8\") as csvfile:\n",
    "    categories = csvfile.readline().split(\",\")\n",
    "    tweetreader = csv.reader(csvfile, delimiter=',')\n",
    "    counter = 0\n",
    "    for row in tweetreader:\n",
    "        tweet = dict(zip(categories, row))\n",
    "        if tweet['language'] == 'English':\n",
    "            tweets.append(tweet['content'])\n",
    "            labels.append(1)\n",
    "            counter += 1\n",
    "        if counter > len_train:\n",
    "            break\n",
    "csvfile.close()\n",
    "\n",
    "\n",
    "\n",
    "for line in lines:\n",
    "    tweets.append(line)\n",
    "    labels.append(0)\n",
    "\n",
    "f.close()\n",
    "            \n",
    "tweets_to_labels = dict(zip(tweets, labels))\n",
    "random.shuffle(tweets)\n",
    "\n",
    "actual = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    actual.append(tweets_to_labels[tweet])\n",
    "data=pd.DataFrame()\n",
    "data['Text']=tweets\n",
    "data['labels']=actual\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7972\n",
       "1    1001\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8973</td>\n",
       "      <td>8973.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8622</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>255</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.314838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Text       labels\n",
       "count   8973  8973.000000\n",
       "unique  8622          NaN\n",
       "top       \\n          NaN\n",
       "freq     255          NaN\n",
       "mean     NaN     0.111557\n",
       "std      NaN     0.314838\n",
       "min      NaN     0.000000\n",
       "25%      NaN     0.000000\n",
       "50%      NaN     0.000000\n",
       "75%      NaN     0.000000\n",
       "max      NaN     1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "- Convsersion to lower case\n",
    "- Removal Punctuation\n",
    "- Tokenization of text using word_tokenize\n",
    "- Removing stop words\n",
    "- Stemming using Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_majority = train_clean[train_clean.label==0]\n",
    "# train_minority = train_clean[train_clean.label==1]\n",
    "# train_minority_upsampled = resample(train_minority, \n",
    "#                                  replace=True,    \n",
    "#                                  n_samples=len(train_majority),   \n",
    "#                                  random_state=123)\n",
    "# train_upsampled = pd.concat([train_minority_upsampled, train_majority])\n",
    "# train_upsampled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @GStephanopoulos  Bernie Sanders on your show ...\n",
       "1    It's 1:00am PST/4:00am EST. Do you know where ...\n",
       "2                                         #DumpTrump\\n\n",
       "3    Look at this bullshit!��� SO topical! everybod...\n",
       "4    I was a #BasementDwellers. Lived out of my car...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text']=df['Text'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    words = word_tokenize(text_p)\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "#     porter=PorterStemmer()\n",
    "#     stem=porter.stem(filtered_words)\n",
    "        \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x:preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [gstephanopoulos, bernie, sanders, show, talki...\n",
       "1    [100am, pst400am, est, know, child, aka, reald...\n",
       "2                                          [dumptrump]\n",
       "3    [look, bullshit���, topical, everybody, talkin...\n",
       "4    [basementdwellers, lived, car, home, thanks, w...\n",
       "5                                      [foxnewssunday]\n",
       "6    [10, emotional, abuse, tactics, trump, blatant...\n",
       "7    [nytimes, illegally, revealing, trump, may, ta...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter=PorterStemmer()\n",
    "\n",
    "def stem(words):\n",
    "    stem_sentence=[]\n",
    "    for word in words:\n",
    "#         stem_words.append(porter.stem(word))\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x:stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    gstephanopoulo berni sander show talk trump di...\n",
       "1    100am pst400am est know child aka realdonaldtr...\n",
       "2                                           dumptrump \n",
       "3    look bullshit��� topic everybodi talk policebr...\n",
       "4    basementdwel live car home thank work amp supp...\n",
       "5                                       foxnewssunday \n",
       "6    10 emot abus tactic trump blatantli use first ...\n",
       "7    nytim illeg reveal trump may taken 950m loss a...\n",
       "8    must denounc trumpism polit mr ryan disturb se...\n",
       "9    usa today exclus hundr alleg donald trump ’ pa...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',  TfidfTransformer()),\n",
    "    ('lr', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'],df['labels'],test_size=0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4033613445378152"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pipeline_lr.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',  TfidfTransformer()),\n",
    "    ('lr', RandomForestClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6501766784452297"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pipeline_rf.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning using GridSearch CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline\n",
    "pipeline_rf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',  TfidfTransformer()),\n",
    "    ('classifier', RandomForestClassifier()),\n",
    "])\n",
    "# Create dictionary with candidate learning algorithms and their hyperparameters\n",
    "grid_param = [\n",
    "#                 {\"classifier\": [LogisticRegression()],\n",
    "#                  \"classifier__penalty\": ['l2','l1'],\n",
    "#                  \"classifier__C\": np.logspace(0, 4, 10)\n",
    "#                  },\n",
    "                {\"classifier\": [LogisticRegression()],\n",
    "                 \"classifier__penalty\": ['l2'],\n",
    "                 \"classifier__C\": np.logspace(0, 4, 10),\n",
    "                 \"classifier__solver\":['newton-cg','saga','sag','liblinear'] ##This solvers don't allow L1 penalty\n",
    "                 },\n",
    "                {\"classifier\": [RandomForestClassifier()],\n",
    "                 \"classifier__n_estimators\": [10, 100, 1000],\n",
    "                 \"classifier__max_depth\":[5,8,15,25,30,None],\n",
    "                 \"classifier__min_samples_leaf\":[1,2,5,10,15,100],\n",
    "                 \"classifier__max_leaf_nodes\": [2, 5,10]}]\n",
    "# create a gridsearch of the pipeline, the fit the best model\n",
    "gridsearch = GridSearchCV(pipeline_rf, grid_param, cv=5, verbose=0,n_jobs=-1) # Fit grid search\n",
    "best_model = gridsearch.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=3593.813663804626, solver='saga'))])\n",
      "The mean accuracy of the model is: 0.950974930362117\n"
     ]
    }
   ],
   "source": [
    "print(best_model.best_estimator_)\n",
    "print(\"The mean accuracy of the model is:\",best_model.score(X_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
